# Word embeddings.


glove_300d_filtered {
  path = embeddings/glove.840B.300d.txt.filtered
  size = 300
  format = txt
  lowercase = false
}


glove_300d_2w {
  path = embeddings/glove_50_300_2.txt
  size = 300
  format = txt
  lowercase = false
}


# Main configuration.
best {
  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  top_span_ratio = 0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab_old.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualizer = lstm
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  lm_layers = 3
  lm_size = 1024
  sva = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  lstm_dropout_rate = 0.4
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  const_weight = 0  # 0.1
  ner_weight = 0  # 0.1

  eval_frequency = 1000
  report_frequency = 250
  log_root = logs
  eval_sleep_secs = 600
}

mtl_best = ${best} {
  char_embedding_size = 8
  contextualization_layers = 3
  contextualization_size = 200

  use_features = true
  use_metadata = true
  model_heads = true
  task_heads = false

  max_arg_width = 30
  argument_ratio = 0.8
  predicate_ratio = 0.4
  mention_ratio = 0.4

  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  lstm_dropout_rate = 0.4
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  num_attention_heads = 1
  span_score_weight = 0.0

  eval_sleep_secs = 1200


  coref_loss = "mention_rank"
  enforce_srl_constraint = false
  filter_v_args = true
  use_gold_predicates = false
  srl_weight = 0
  ner_weight = 1.0
  coref_weight = 1.0
  const_weight = 0.0

  batch_size = 40
  max_tokens_per_batch = 700

  # Updated dataset.
  train_path = train.english.mtl.jsonlines
  eval_path = dev.english.mtl.jsonlines
  lm_layers = 3
  lm_size = 1024
  main_metrics = srl_coref_ner

}

# Scientific KG experiments


scientific_entity = ${mtl_best} {
  include_c_v = false
  ner_labels = ["Task", "Generic", "Metric", "Material", "OtherScientificTerm", "Method"]
  relation_labels = ["COMPARE", "PART-OF", "FEATURE-OF", "USED-FOR", "CONJUNCTION", "HYPONYM-OF", "EVALUATE-FOR"]
  coref_weight = 0.0
  ner_weight = 1.0
  relation_weight = 0.0
  max_arg_width = 5
  contextualization_layers = 1
  main_metrics = ner

  ner_conll_eval_path = ""
  eval_frequency = 50
  report_frequency = 25
  eval_sleep_secs = 10

  filter_reverse_relations = true
  entity_ratio = 0.5
  coref_weight = 0.5
  ner_weight = 0.5 
  max_arg_width = 10
  main_metrics = coref_ner
  use_metadata = false
}


scientific_elmo = ${scientific_entity} {
  coref_weight = 0.33
  ner_weight = 0.33
  relation_weight = 0.33
  main_metrics = coref_ner_relations
  batch_size = 10
  mention_ratio = 0.3
  entity_ratio = 0.3
  max_arg_width = 8
  lm_path = "./data/processed_data/elmo/train.hdf5"
  lm_path_dev = "./data/processed_data/elmo/luanyi_input/miachiave.hdf5"
  train_path = "./data/processed_data/json/train.json"
  eval_path = "./data/processed_data/json/luanyi_input/miachiave.json"
}

scientific_best_ner = ${scientific_elmo} {
learning_rate = 0.0005
coref_weight = 1.0
ner_weight = 0.2
main_metrics = ner
}

scientific_best_coref = ${scientific_elmo} {
learning_rate = 0.0005
main_metrics = coref
}

scientific_best_relation = ${scientific_elmo} {
  ffn = 250
  coref_weight = 0.3
  dropout_rate = 0.4
  entity_ratio = 0.4
  ner_weight = 0
  relation_weight = 1.0
  learning_rate = 0.001
  main_metrics = relations
  output_path = ./output.json
}
